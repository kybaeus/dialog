{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Warning\n",
    "This file is currently a work in progress. Expect instability, inconsistency, and ugly code.\n",
    "@W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"../../data\"\n",
    "\n",
    "# Delete me, eventually\n",
    "data_path = DATA_DIRECTORY + '/dialog-babi-task1/'\n",
    "training_file = data_path + 'dialog-babi-task1-API-calls-trn-workspace.txt'\n",
    "dev_file = data_path + 'dialog-babi-task1-API-calls-dev-workspace.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functional import seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial object constructors\n",
    "class Workspace():\n",
    "    def __init__(self, tree):\n",
    "        # assert tree is array\n",
    "        self.tree = tree\n",
    "        \n",
    "    def add(address, content):\n",
    "        assert isinstance(address, (str))\n",
    "        assert isinstance(content, (str))\n",
    "        \n",
    "        def _add(tree):\n",
    "            if not isinstance(tree, (list)):\n",
    "                return tree\n",
    "            elif tree[0] == address:\n",
    "                return tree.concat([[content]])\n",
    "            else:\n",
    "                return map(tree, _add)\n",
    "        \n",
    "        return Workspace(_add(self.tree))\n",
    "\n",
    "    def update(self, action):\n",
    "        if action.what == 'INIT':\n",
    "            return Workspace(['root'])\n",
    "        elif action.what == 'MSG':\n",
    "            return self\n",
    "        elif action.what == 'ADD':\n",
    "            tmp = action.content.split(' ')\n",
    "            assert (len(tmp) == 2)\n",
    "            address = tmp[0]\n",
    "            address = tmp[1]\n",
    "            return this.add(address, content)\n",
    "        \n",
    "    # TODO: Figure out how to flattenDeep    \n",
    "    def to_words():\n",
    "        return \"NOT IMPLEMENTED YET\"\n",
    "    def to_tree():\n",
    "        return this.tree\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Action():\n",
    "    def __init__(self, who, what, content=False):\n",
    "        self.who = who\n",
    "        self.what = what\n",
    "        self.content = content\n",
    "        \n",
    "    def to_words(self):\n",
    "        return [f\"{self.who}:\", self.what] + (self.content.split(' ') if self.content else [])\n",
    "        \n",
    "    def to_string(self):\n",
    "        if self.content:\n",
    "            return f'{self.who}: {self.what} {self.content}'\n",
    "        else:\n",
    "            return f'{self.who}: {self.what}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def files_to_words(filenames):\n",
    "     return (seq(filenames)\n",
    "                .map(lambda filename: open(filename).read().split())\n",
    "                .flatten()\n",
    "                .distinct()\n",
    "                .sorted()\n",
    "                .to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def string_to_action(s):\n",
    "    who = s[0]\n",
    "    i = s.index(\" \")\n",
    "    line = s[i+1:]\n",
    "    j = line.index(\" \")\n",
    "    what = line[0:j]\n",
    "    content = line[j+1:]\n",
    "    return Action(who, what, content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y = file_to_dialogs(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_dialogs(filename):\n",
    "    init_entry = {\n",
    "        \"workspace\": Workspace([]),\n",
    "        \"action\": Action(who='A', what='INIT')\n",
    "    }\n",
    "    dialog_list = []\n",
    "    current_workspace = init_entry[\"workspace\"] \n",
    "    \n",
    "    for dialog in open(filename).read().split('\\n\\n'):\n",
    "        action_list = [init_entry]\n",
    "        for action in dialog.split(\"\\n\"):\n",
    "            action_list.append({\"workspace\": current_workspace,\n",
    "                                \"action\": string_to_action(action)})\n",
    "        dialog_list.append(action_list)\n",
    "        \n",
    "    return dialog_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_words = [\n",
    "  '^', '$',            # start/end markers\n",
    "  'INIT', 'WAIT',      # action types not in data\n",
    "  'NO_OUTPUT',         # filler when copy or match word is not available\n",
    "  'NO_USER_ACTION',    # initial value of lastUserAction\n",
    "  'NO_WORKSPACE_WORD'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def load_babi_data():\n",
    "    data_path = DATA_DIRECTORY + '/dialog-babi-task1/'\n",
    "    training_file = data_path + 'dialog-babi-task1-API-calls-trn-workspace.txt'\n",
    "    dev_file = data_path + 'dialog-babi-task1-API-calls-dev-workspace.txt'\n",
    "    types_and_tokens = [\n",
    "            { \"type\": 'cuisine', \"tokens\": ['french', 'italian', 'british', 'spanish', 'indian'] },\n",
    "            { \"type\": 'location', \"tokens\": ['rome', 'london', 'bombay', 'paris', 'madrid'] },\n",
    "            { \"type\": 'price', \"tokens\": ['cheap', 'moderate', 'expensive'] },\n",
    "            { \"type\": 'people', \"tokens\": ['two', 'four', 'six', 'eight'] },\n",
    "        ]\n",
    "    \n",
    "    data = {\n",
    "      \"training\": file_to_dialogs(training_file),\n",
    "      \"dev\": file_to_dialogs(dev_file),\n",
    "      \"words\": files_to_words([training_file]) + special_words,\n",
    "      \"types_and_tokens\": types_and_tokens\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_one_hot(vocab_size):\n",
    "    return ([[int(i == j) for i in range(vocab_size)] for j in range(vocab_size)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def action_to_one_hot(action):\n",
    "    return ((seq(['^'] + action.to_words() + ['$'])\n",
    "                .map(lambda w: data['words'].index(w))\n",
    "                .map(lambda i: one_hot[i])\n",
    "                .to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def action_to_idx(action):\n",
    "    return ((seq(['^'] + action.to_words() + ['$'])\n",
    "                .map(lambda w: data['words'].index(w))\n",
    "                .to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_TENSOR_SIZE = 23 #TODO: Refactor to determine automatically\n",
    "def pad_tensor(tensor):\n",
    "    m = MAX_TENSOR_SIZE\n",
    "    a = tensor\n",
    "    return a + [([0 for i in range(vocab_size)]) for j in range(m - len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_TENSOR_SIZE = 23 #TODO: Refactor to determine automatically\n",
    "def pad_word_tensor(tensor):\n",
    "    m = MAX_TENSOR_SIZE\n",
    "    a = tensor\n",
    "    return a + [vocab_size-1 for j in range(m - len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def actions_to_one_hot(data):\n",
    "    new_data = (seq(data)\n",
    "                .map(lambda a: a['action'])\n",
    "                .map(lambda a: action_to_one_hot(a))\n",
    "                .map(pad_tensor)\n",
    "                .map(torch.LongTensor))\n",
    "    return new_data\n",
    "\n",
    "                #.map(action_to_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def actions_to_idx(data):\n",
    "    new_data = (seq(data)\n",
    "                .map(lambda a: a['action'])\n",
    "                .map(lambda a: action_to_idx(a))\n",
    "                .map(pad_word_tensor)\n",
    "                .map(torch.LongTensor))\n",
    "    return new_data\n",
    "\n",
    "                #.map(action_to_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dialogs_to_one_hot(dialogs):\n",
    "    out_data = []\n",
    "    for dialog in dialogs:\n",
    "        out = torch.stack(list(actions_to_one_hot(dialog)))\n",
    "        out_data.append(out)\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dialogs_to_idx(dialogs):\n",
    "    out_data = []\n",
    "    for dialog in dialogs:\n",
    "        out = torch.stack(list(actions_to_idx(dialog)))\n",
    "        out_data.append(out)\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data = load_babi_data()\n",
    "\n",
    "# Ensure it looks ok\n",
    "#data['training'][100][\"action\"].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A: MSG api_call italian london eight cheap'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['training'][1][-1]['action'].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(data[\"words\"])\n",
    "#one_hot = create_one_hot(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert our data into a list of s\n",
    "#training_data = dialogs_to_one_hot(data['training'])\n",
    "training_data = (dialogs_to_idx(data['training']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-005e7af93950>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-005e7af93950>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    idx_to_words(training_data[][7])\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "idx_to_words(training_data[][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def idx_to_words(idxs): return list(map(lambda i: data['words'][int(i)], idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# WARNING!\n",
    "Everything below this cell is currently non-functional. If it compiles, I'll be rather surprised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_size, h_size, n_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.h_size = h_size\n",
    "        self.x_size = x_size\n",
    "        self.cell = nn.GRU(h_size, h_size)\n",
    "        self.embedding = nn.Embedding(x_size, h_size)\n",
    "        \n",
    "        #self.embedding = nn.Embedding(x_size, h_size)\n",
    "\n",
    "    # x: input\n",
    "    # h: hidden state\n",
    "    def forward(self, input, h):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, h = self.cell(output, h)\n",
    "        return output, h\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, 1, self.h_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderMock(nn.Module):\n",
    "    def __init__(self, x_size, h_size, n_layers=1):\n",
    "        super(DecoderMock, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.h_size = h_size\n",
    "        self.x_size = x_size\n",
    "        self.cell = nn.GRU(h_size, h_size)\n",
    "        self.out = nn.Linear(h_size, x_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.embedding = nn.Embedding(x_size, h_size)\n",
    "\n",
    "    # x: input\n",
    "    # h: hidden state\n",
    "    def forward(self, input, h):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, h = self.cell(output, h)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, h\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, 1, self.h_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One day, we will be a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_variables = list(map(lambda d: Variable(d), training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = 1000\n",
    "encodern = Encoder(vocab_size, hidden_size).cuda()\n",
    "decodern = DecoderMock(vocab_size, hidden_size).cuda()\n",
    "plot_losses = []\n",
    "plot_every = 1\n",
    "plot_loss_total = 0\n",
    "\n",
    "encoder_optimizer = torch.optim.SGD(encodern.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decodern.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss() \n",
    "loss = 0\n",
    "\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epochs():\n",
    "    for epoch in range(e):\n",
    "        plot_loss_total = 0 \n",
    "        loss = train(encodern, decodern, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        plot_loss_total += loss[0]\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0 \n",
    "        print(plot_loss_avg)\n",
    "    #showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIALOG: 0\n",
      "DIALOG: 1\n",
      "DIALOG: 2\n",
      "DIALOG: 3\n",
      "DIALOG: 4\n",
      "DIALOG: 5\n",
      "DIALOG: 6\n",
      "DIALOG: 7\n",
      "DIALOG: 8\n",
      "DIALOG: 9\n",
      "DIALOG: 10\n",
      "DIALOG: 11\n",
      "DIALOG: 12\n",
      "DIALOG: 13\n",
      "DIALOG: 14\n",
      "DIALOG: 15\n",
      "DIALOG: 16\n",
      "DIALOG: 17\n",
      "DIALOG: 18\n",
      "DIALOG: 19\n"
     ]
    }
   ],
   "source": [
    "train_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(encodern, decodern, encoder_optimizer, decoder_optimizer, criterion, offset=0):\n",
    "   \n",
    "    encodern.zero_grad()\n",
    "    decodern.zero_grad()\n",
    "\n",
    "    encoder_hidden = encodern.init_hidden().cuda()\n",
    "    decoder_hidden = decodern.init_hidden().cuda()\n",
    "\n",
    "    TARGET_LENGTH = 20\n",
    "    BATCH_SIZE = 1\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(1, encodern.h_size) )\n",
    "\n",
    "    sos = data['words'].index('^')\n",
    "    eos = data['words'].index('$')\n",
    "\n",
    "    encoder_input = Variable(torch.LongTensor([sos]).cuda())\n",
    "    decoder_input = Variable(torch.LongTensor([sos]).cuda())\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    #training_variables = list(map(lambda d: Variable(d), training_data))\n",
    "\n",
    "    # This may require a little explanation\n",
    "    # we use history as a cache of messages\n",
    "    # at every \"action\" in the dialogs we attempt\n",
    "    # to predict it given the previously given history\n",
    "    #for dialog in training_data[offset:BATCH_SIZE+offset]: \n",
    "    for it, dialog in enumerate(training_data): \n",
    "        gc.collect()\n",
    "        print(\"DIALOG: \" + str(it))\n",
    "\n",
    "        # I'm not sure of a better way to optimize this, due to our sequences being variable length\n",
    "        history = []\n",
    "        for action in dialog:\n",
    "\n",
    "            # Append our action to our history\n",
    "            history = history + [action]\n",
    "\n",
    "            # If we have only one action we can't have both training and test\n",
    "            if len(history) < 2: continue\n",
    "\n",
    "            # Iterate thruugh every message(action) in the history other than the current\n",
    "            for message in history[:-1]:\n",
    "                for word in message:\n",
    "                    encoder_input = Variable(torch.LongTensor([word]).cuda())\n",
    "                    encoder_outputs, encoder_hidden = encodern(encoder_input, encoder_hidden)\n",
    "\n",
    "            decoder_hidden = encoder_hidden # Share our hidden state\n",
    "\n",
    "            # Seed our decoder with the start of sentence token\n",
    "\n",
    "            # Start Decoding Cycle\n",
    "            # Note that we use its own predictions as the next input\n",
    "                # Q: Should I add teacher forcing?\n",
    "            \n",
    "            int_d_outputs = []\n",
    "            for i in range(TARGET_LENGTH):\n",
    "                decoder_output, decoder_hidden = decodern(decoder_input, decoder_hidden)\n",
    "                int_d_outputs.append(decoder_output.data)\n",
    "            \n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                ni = topi[0][0]\n",
    "                decoder_input = Variable(torch.LongTensor([[ni]])).cuda()\n",
    " \n",
    "                loss += criterion(decoder_output[-1],\n",
    "                                  Variable(torch.LongTensor([history[-1][i]]).cuda()))\n",
    "            \n",
    "                if ni == eos:\n",
    "                    break    \n",
    "                    \n",
    "            \n",
    "    loss.backward()\n",
    "     \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step() \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Variable data has to be a tensor, but got Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-1bdf3439ffd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Variable data has to be a tensor, but got Variable"
     ]
    }
   ],
   "source": [
    "criterion(Variable(a, requires_grad=True),b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Ideal system for s2s architecture\n",
    "\n",
    "[w_1] ... [w_n] -> (encoder) -> [e_1] ... [e_n] -> (decoder) -> [wp_1] ... [wp_n] <compare> [wt_1] ... [wt_n] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is my loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement initialization\n",
    "w\n",
    "\n",
    "# Whether this is a good idea is still in question\n",
    "attentional_net = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(2, 1),\n",
    "                    torch.nn.Tanh())\n",
    "\n",
    "# Whether this is a good idea is still in question\n",
    "decoder_output_net = torch.nn.Sequential(\n",
    "                        torch.nn.Softmax())\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_size, h_size, n_layers=1):\n",
    "        self.n_layers = n_layers\n",
    "        self.h_size = h_size\n",
    "        self.cell = nn.GRU(x_size, h_size)\n",
    "\n",
    "        self.out = nn.Linear(h_size, )\n",
    "        self.attention = torch.nn.Sequential(torch.nn.Linear(2 * x_size, 1),\n",
    "                                             torch.nn.Tanh())\n",
    "\n",
    "    def forward(self, decoder_state, encoder_states):\n",
    "        # Combine State\n",
    "        combinedState = lambda encoder_state, z: self.attention(torch.cat(decoder_state, encoder_state), 0)\n",
    "\n",
    "        # This is terrible and I feel dirty\n",
    "        zero_states = torch.zeros(encoder_states.size()[0])\n",
    "\n",
    "        # Create attention vector\n",
    "        attention_vector = T.softmax(torch.Tensor.map_(encoder_states, zero_states,\n",
    "                                                       combinedState))\n",
    "\n",
    "        final = encoder_states * torch.transpose(attention_vector.unsqueeze(0), 0, 1).expand(encoder_states.size())\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output, h = self.cell(output, h)\n",
    "        return output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def go(e):\n",
    "    encodern = Encoder(vocab_size, hidden_size).cuda()\n",
    "    decodern = DecoderMock(vocab_size, hidden_size).cuda()\n",
    "    plot_losses = []\n",
    "    plot_every = 1\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = torch.optim.SGD(encodern.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.SGD(decodern.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss() \n",
    "    loss = 0\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(e):\n",
    "        loss = build_and_train(encodern, decodern, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        plot_loss_total += loss[0]\n",
    "        \n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0 \n",
    "        print(plot_loss_avg)\n",
    "    #showPlot(plot_losses)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_and_train() missing 5 required positional arguments: 'encodern', 'decodern', 'encoder_optimizer', 'decoder_optimizer', and 'criterion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-428-c1f3766d25fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: build_and_train() missing 5 required positional arguments: 'encodern', 'decodern', 'encoder_optimizer', 'decoder_optimizer', and 'criterion'"
     ]
    }
   ],
   "source": [
    "build_and_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
