{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Warning\n",
    "This file is currently a work in progress. Expect instability, inconsistency, and ugly code.\n",
    "@W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"../../data\"\n",
    "\n",
    "# Delete me, eventually\n",
    "data_path = DATA_DIRECTORY + '/dialog-babi-task1/'\n",
    "training_file = data_path + 'dialog-babi-task1-API-calls-trn-workspace.txt'\n",
    "dev_file = data_path + 'dialog-babi-task1-API-calls-dev-workspace.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functional import seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial object constructors\n",
    "class Workspace():\n",
    "    def __init__(self, tree):\n",
    "        # assert tree is array\n",
    "        self.tree = tree\n",
    "        \n",
    "    def add(address, content):\n",
    "        assert isinstance(address, (str))\n",
    "        assert isinstance(content, (str))\n",
    "        \n",
    "        def _add(tree):\n",
    "            if not isinstance(tree, (list)):\n",
    "                return tree\n",
    "            elif tree[0] == address:\n",
    "                return tree.concat([[content]])\n",
    "            else:\n",
    "                return map(tree, _add)\n",
    "        \n",
    "        return Workspace(_add(self.tree))\n",
    "\n",
    "    def update(self, action):\n",
    "        if action.what == 'INIT':\n",
    "            return Workspace(['root'])\n",
    "        elif action.what == 'MSG':\n",
    "            return self\n",
    "        elif action.what == 'ADD':\n",
    "            tmp = action.content.split(' ')\n",
    "            assert (len(tmp) == 2)\n",
    "            address = tmp[0]\n",
    "            address = tmp[1]\n",
    "            return this.add(address, content)\n",
    "        \n",
    "    # TODO: Figure out how to flattenDeep    \n",
    "    def to_words():\n",
    "        return \"NOT IMPLEMENTED YET\"\n",
    "    def to_tree():\n",
    "        return this.tree\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Action():\n",
    "    def __init__(self, who, what, content=False):\n",
    "        self.who = who\n",
    "        self.what = what\n",
    "        self.content = content\n",
    "        \n",
    "    def to_words(self):\n",
    "        return [f\"{self.who}:\", self.what] + (self.content.split(' ') if self.content else [])\n",
    "        \n",
    "    def to_string(self):\n",
    "        if self.content:\n",
    "            return f'{self.who}: {self.what} {self.content}'\n",
    "        else:\n",
    "            return f'{self.who}: {self.what}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def files_to_words(filenames):\n",
    "     return (seq(filenames)\n",
    "                .map(lambda filename: open(filename).read().split())\n",
    "                .flatten()\n",
    "                .distinct()\n",
    "                .sorted()\n",
    "                .to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def string_to_action(s):\n",
    "    who = s[0]\n",
    "    i = s.index(\" \")\n",
    "    line = s[i+1:]\n",
    "    j = line.index(\" \")\n",
    "    what = line[0:j]\n",
    "    content = line[j+1:]\n",
    "    return Action(who, what, content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y = file_to_dialogs(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_dialogs(filename):\n",
    "    init_entry = {\n",
    "        \"workspace\": Workspace([]),\n",
    "        \"action\": Action(who='A', what='INIT')\n",
    "    }\n",
    "    dialog_list = []\n",
    "    current_workspace = init_entry[\"workspace\"] \n",
    "    \n",
    "    for dialog in open(filename).read().split('\\n\\n'):\n",
    "        action_list = [init_entry]\n",
    "        for action in dialog.split(\"\\n\"):\n",
    "            action_list.append({\"workspace\": current_workspace,\n",
    "                                \"action\": string_to_action(action)})\n",
    "        dialog_list.append(action_list)\n",
    "        \n",
    "    return dialog_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_words = [\n",
    "  '^', '$',            # start/end markers\n",
    "  'INIT', 'WAIT',      # action types not in data\n",
    "  'NO_OUTPUT',         # filler when copy or match word is not available\n",
    "  'NO_USER_ACTION',    # initial value of lastUserAction\n",
    "  'NO_WORKSPACE_WORD'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def load_babi_data():\n",
    "    data_path = DATA_DIRECTORY + '/dialog-babi-task1/'\n",
    "    training_file = data_path + 'dialog-babi-task1-API-calls-trn-workspace.txt'\n",
    "    dev_file = data_path + 'dialog-babi-task1-API-calls-dev-workspace.txt'\n",
    "    types_and_tokens = [\n",
    "            { \"type\": 'cuisine', \"tokens\": ['french', 'italian', 'british', 'spanish', 'indian'] },\n",
    "            { \"type\": 'location', \"tokens\": ['rome', 'london', 'bombay', 'paris', 'madrid'] },\n",
    "            { \"type\": 'price', \"tokens\": ['cheap', 'moderate', 'expensive'] },\n",
    "            { \"type\": 'people', \"tokens\": ['two', 'four', 'six', 'eight'] },\n",
    "        ]\n",
    "    \n",
    "    data = {\n",
    "      \"training\": file_to_dialogs(training_file),\n",
    "      \"dev\": file_to_dialogs(dev_file),\n",
    "      \"words\": files_to_words([training_file]) + special_words,\n",
    "      \"types_and_tokens\": types_and_tokens\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_one_hot(vocab_size):\n",
    "    return ([[int(i == j) for i in range(vocab_size)] for j in range(vocab_size)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def action_to_one_hot(action):\n",
    "    return ((seq(['^'] + action.to_words() + ['$'])\n",
    "                .map(lambda w: data['words'].index(w))\n",
    "                .map(lambda i: one_hot[i])\n",
    "                .to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def action_to_idx(action):\n",
    "    return ((seq(['^'] + action.to_words() + ['$'])\n",
    "                .map(lambda w: data['words'].index(w))\n",
    "                .to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_TENSOR_SIZE = 23 #TODO: Refactor to determine automatically\n",
    "def pad_tensor(tensor):\n",
    "    m = MAX_TENSOR_SIZE\n",
    "    a = tensor\n",
    "    return a + [([0 for i in range(vocab_size)]) for j in range(m - len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_TENSOR_SIZE = 23 #TODO: Refactor to determine automatically\n",
    "def pad_word_tensor(tensor):\n",
    "    m = MAX_TENSOR_SIZE\n",
    "    a = tensor\n",
    "    return a + [vocab_size-1 for j in range(m - len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def actions_to_one_hot(data):\n",
    "    new_data = (seq(data)\n",
    "                .map(lambda a: a['action'])\n",
    "                .map(lambda a: action_to_one_hot(a))\n",
    "                .map(pad_tensor)\n",
    "                .map(torch.LongTensor))\n",
    "    return new_data\n",
    "\n",
    "                #.map(action_to_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def actions_to_idx(data):\n",
    "    new_data = (seq(data)\n",
    "                .map(lambda a: a['action'])\n",
    "                .filter(lambda a: a.what == \"MSG\")\n",
    "                .map(lambda a: action_to_idx(a)))\n",
    "                #.map(pad_word_tensor)\n",
    "                #.map(torch.LongTensor))\n",
    "    return new_data\n",
    "\n",
    "                #.map(action_to_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dialogs_to_one_hot(dialogs):\n",
    "    out_data = []\n",
    "    for dialog in dialogs:\n",
    "        out = torch.stack(list(actions_to_one_hot(dialog)))\n",
    "        out_data.append(out)\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dialogs_to_idx(dialogs):\n",
    "    out_data = []\n",
    "    for dialog in dialogs:\n",
    "       # out = torch.stack(list(actions_to_idx(dialog)))\n",
    "        out = list(actions_to_idx(dialog))\n",
    "        out_data.append(out)\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data = load_babi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A: MSG api_call italian london eight cheap'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['training'][1][-1]['action'].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(data[\"words\"])\n",
    "#one_hot = create_one_hot(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert our data into a list of s\n",
    "\n",
    "#training_data = dialogs_to_one_hot(data['training'])\n",
    "training_data = (dialogs_to_idx(data['training']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def idx_to_words(idxs): return list(map(lambda i: data['words'][int(i)], idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_size, h_size, n_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.h_size = h_size\n",
    "        self.x_size = x_size\n",
    "        self.cell = nn.GRU(h_size, h_size)\n",
    "        self.embedding = nn.Embedding(x_size, h_size)\n",
    "        \n",
    "        #self.embedding = nn.Embedding(x_size, h_size)\n",
    "\n",
    "    # x: input\n",
    "    # h: hidden state\n",
    "    def forward(self, input, h):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, h = self.cell(output, h)\n",
    "        return output, h\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, 1, self.h_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderMock(nn.Module):\n",
    "    def __init__(self, x_size, h_size, n_layers=1):\n",
    "        super(DecoderMock, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.h_size = h_size\n",
    "        self.x_size = x_size\n",
    "        self.cell = nn.GRU(h_size, h_size)\n",
    "        self.out = nn.Linear(h_size, x_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.embedding = nn.Embedding(x_size, h_size)\n",
    "\n",
    "    # x: input\n",
    "    # h: hidden state\n",
    "    def forward(self, input, h):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, h = self.cell(output, h)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, h\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, 1, self.h_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement initialization\n",
    "\n",
    "# Whether this is a good idea is still in question\n",
    "attentional_net = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(2, 1),\n",
    "                    torch.nn.Tanh())\n",
    "\n",
    "# Whether this is a good idea is still in question\n",
    "decoder_output_net = torch.nn.Sequential(\n",
    "                        torch.nn.Softmax())\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_size, h_size, n_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.h_size = h_size\n",
    "        self.cell = nn.GRU(x_size, h_size)\n",
    "        self.embedding = nn.Embedding(x_size, h_size)\n",
    "\n",
    "        self.decoder_out = torch.nn.Sequential(torch.nn.Linear(h_size + x_size + h_size, x_size),\n",
    "                                               torch.nn.LogSoftmax())\n",
    "        self.attention = torch.nn.Sequential(torch.nn.Linear(2 * x_size, 1),\n",
    "                                             torch.nn.Tanh())\n",
    "\n",
    "    def forward(self, decoder_state, encoder_states):\n",
    "        \n",
    "        embed = self.embedding(decoder_state).view(1, 1, -1)\n",
    "        decoder_state = embed\n",
    "        \n",
    "        # Combine State\n",
    "        combinedState = lambda encoder_state, z: self.attention(torch.cat(decoder_state, encoder_state), 0)\n",
    "\n",
    "        # This is terrible and I feel dirty\n",
    "        zero_states = torch.zeros(encoder_states.size()[0])\n",
    "\n",
    "        # Create attention vector\n",
    "        attention_vector = F.softmax(torch.Tensor.map_(encoder_states.data, zero_states.data,\n",
    "                                                       combinedState))\n",
    "\n",
    "        \n",
    "        final = encoder_states * torch.transpose(attention_vector.unsqueeze(0), 0, 1).expand(encoder_states.size())\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            decoder_state, h = self.cell(decoder_state, h)\n",
    "        decoder_state = self.decoder_out(torch.concat(decoder_state, embed, attention_vector))\n",
    "        return decoder_state, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_states):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attention\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One day, we will be a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix for CUDNN memory leak \n",
    "# https://discuss.pytorch.org/t/tracking-down-a-suspected-memory-leak/1130/39\n",
    "#torch.backends.cudnn.enabled = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = 1000\n",
    "encodern = Encoder(vocab_size, hidden_size)\n",
    "decodern = DecoderMock(vocab_size, hidden_size)\n",
    "#decodern = Decoder(vocab_size, hidden_size)\n",
    "plot_losses = []\n",
    "plot_every = 1\n",
    "plot_loss_total = 0\n",
    "\n",
    "encoder_optimizer = torch.optim.SGD(encodern.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.SGD(decodern.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss() \n",
    "loss = 0\n",
    "\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "get_time = lambda: strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk_by(in_list, by): return (seq(in_list).grouped(by).map(lambda l: list(l)).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epochs(e=100, batch_size=10):\n",
    "    for ep, epoch in enumerate(range(e)):\n",
    "        print(\"Epoch:\" +  str(ep))\n",
    "        torch.save(encodern, open(\"encoder\" + get_time() + \"-epoch-\" + str(ep),'wb'))\n",
    "        torch.save(decodern, open(\"decoder\" + get_time() + \"-epoch-\" + str(ep),'wb'))\n",
    "        plot_loss_total = 0 \n",
    "        #for i in range(batch)\n",
    "        encoder_hidden = encodern.init_hidden()\n",
    "        decoder_hidden = decodern.init_hidden()\n",
    "\n",
    "            \n",
    "        for i, batch in enumerate(chunk_by(training_data, batch_size)):\n",
    "            #print(\"Batch: \" + str(i))\n",
    "            loss = train(batch, encodern, decodern, encoder_optimizer, decoder_optimizer, encoder_hidden, decoder_hidden, criterion)\n",
    "            plot_loss_total += loss[0]\n",
    "\n",
    "            if epoch % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0 \n",
    "            print(plot_loss_avg)\n",
    "        torch.save(encodern, open(\"encoder\" + get_time() + \"-epoch-\" + str(ep),'wb'))\n",
    "        torch.save(decodern, open(\"decoder\" + get_time() + \"-epoch-\" + str(ep),'wb'))\n",
    "    #showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dialog_data, encodern, decodern, encoder_optimizer, decoder_optimizer, encoder_hidden, decoder_hidden, criterion):\n",
    "   \n",
    "    encodern.zero_grad()\n",
    "    decodern.zero_grad()\n",
    "\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(1, encodern.h_size))\n",
    "\n",
    "    sos = data['words'].index('^')\n",
    "    eos = data['words'].index('$')\n",
    "\n",
    "    encoder_input = Variable(torch.LongTensor([sos]))\n",
    "    decoder_input = Variable(torch.LongTensor([sos]))\n",
    "\n",
    "    #encoder_input = Variable(torch.LongTensor([word]))\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    #training_variables = list(map(lambda d: Variable(d), training_data))\n",
    "\n",
    "    # This may require a little explanation\n",
    "    # we use history as a cache of messages\n",
    "    # at every \"action\" in the dialogs we attempt\n",
    "    # to predict it given the previously given history\n",
    "    for it, dialog in enumerate(dialog_data): \n",
    "        gc.collect()\n",
    "        print(\"DIALOG: \" + str(it))\n",
    "\n",
    "        # I'm not sure of a better way to optimize this, due to our sequences being variable length\n",
    "        history = []\n",
    "        for action in dialog:\n",
    "\n",
    "            # Append our action to our history\n",
    "            history = history + [action]\n",
    "\n",
    "            # If we have only one action we can't have both training and test\n",
    "            if len(history) < 2: continue\n",
    "\n",
    "            # Iterate thruugh every message(action) in the history other than the current\n",
    "            for message in history[:-1]:\n",
    "                for word in message:\n",
    "                    encoder_word = word\n",
    "                    encoder_input = Variable(torch.LongTensor([word]))\n",
    "                    encoder_outputs, encoder_hidden = encodern(encoder_input, encoder_hidden)\n",
    "\n",
    "            decoder_hidden = encoder_hidden # Share our hidden state\n",
    "\n",
    "            # Seed our decoder with the start of sentence token\n",
    "\n",
    "            # Start Decoding Cycle\n",
    "            # Note that we use its own predictions as the next input\n",
    "                # Q: Should I add teacher forcing?\n",
    "            int_d_outputs = []\n",
    "            for line in history[-1]:\n",
    "                decoder_output, decoder_hidden =  decodern(decoder_input, decoder_hidden)\n",
    "            \n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                ni = topi[0][0]\n",
    "                if random.random() > 0.9:\n",
    "                    decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "                else:\n",
    "                    decoder_input = Variable(torch.LongTensor([[line]]))\n",
    " \n",
    "                int_d_outputs.append(data['words'][ni])\n",
    "                loss += criterion(decoder_output[-1],\n",
    "                                  Variable(torch.LongTensor([line])))\n",
    "            \n",
    "                if ni == eos:\n",
    "                    break    \n",
    "                    \n",
    "            \n",
    "    loss.backward()\n",
    "     \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step() \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type DecoderMock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIALOG: 0\n",
      "DIALOG: 1\n",
      "DIALOG: 2\n",
      "DIALOG: 3\n",
      "DIALOG: 4\n",
      "DIALOG: 5\n",
      "DIALOG: 6\n",
      "DIALOG: 7\n",
      "DIALOG: 8\n",
      "DIALOG: 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-34126afa979c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-8ccb6ea9592a>\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(e, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m#print(\"Batch: \" + str(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencodern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecodern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-083c7e4575fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dialog_data, encodern, decodern, encoder_optimizer, decoder_optimizer, encoder_hidden, decoder_hidden, criterion, offset)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m#print(int_d_outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train our model!\n",
    "train_epochs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# WARNING!\n",
    "Everything below this cell is currently non-functional. If it compiles, I'll be rather surprised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion(Variable(a, requires_grad=True),b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Ideal system for s2s architecture\n",
    "\n",
    "[w_1] ... [w_n] -> (encoder) -> [e_1] ... [e_n] -> (decoder) -> [wp_1] ... [wp_n] <compare> [wt_1] ... [wt_n] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement initialization\n",
    "\n",
    "# Whether this is a good idea is still in question\n",
    "attentional_net = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(2, 1),\n",
    "                    torch.nn.Tanh())\n",
    "\n",
    "# Whether this is a good idea is still in question\n",
    "decoder_output_net = torch.nn.Sequential(\n",
    "                        torch.nn.Softmax())\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_size, h_size, n_layers=1):\n",
    "        self.n_layers = n_layers\n",
    "        self.h_size = h_size\n",
    "        self.cell = nn.GRU(x_size, h_size)\n",
    "\n",
    "        self.out = nn.Linear(h_size, )\n",
    "        self.attention = torch.nn.Sequential(torch.nn.Linear(2 * x_size, 1),\n",
    "                                             torch.nn.Tanh())\n",
    "\n",
    "    def forward(self, decoder_state, encoder_states):\n",
    "        # Combine State\n",
    "        combinedState = lambda encoder_state, z: self.attention(torch.cat(decoder_state, encoder_state), 0)\n",
    "\n",
    "        # This is terrible and I feel dirty\n",
    "        zero_states = torch.zeros(encoder_states.size()[0])\n",
    "\n",
    "        # Create attention vector\n",
    "        attention_vector = T.softmax(torch.Tensor.map_(encoder_states, zero_states,\n",
    "                                                       combinedState))\n",
    "\n",
    "        final = encoder_states * torch.transpose(attention_vector.unsqueeze(0), 0, 1).expand(encoder_states.size())\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output, h = self.cell(output, h)\n",
    "        return output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def go(e):\n",
    "    encodern = Encoder(vocab_size, hidden_size). \n",
    "    decodern = DecoderMock(vocab_size, hidden_size). \n",
    "    plot_losses = []\n",
    "    plot_every = 1\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = torch.optim.SGD(encodern.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.SGD(decodern.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss() \n",
    "    loss = 0\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(e):\n",
    "        loss = build_and_train(encodern, decodern, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        plot_loss_total += loss[0]\n",
    "        \n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0 \n",
    "        print(plot_loss_avg)\n",
    "    #showPlot(plot_losses)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderMock(nn.Module):\n",
    "    def __init__(self, x_size, h_size, n_layers=1):\n",
    "        super(DecoderMock, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.h_size = h_size\n",
    "        self.x_size = x_size\n",
    "        self.cell = nn.GRU(h_size, h_size)\n",
    "        self.out = nn.Linear(h_size, x_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.attention = torch.nn.Sequential(torch.nn.Linear(2 * x_size, 1),\n",
    "                                             torch.nn.Tanh())\n",
    "\n",
    "        self.embedding = nn.Embedding(x_size, h_size)\n",
    "\n",
    "    # x: input\n",
    "    # h: hidden state\n",
    "    def forward(self, input, h, encoder_states):\n",
    "        \n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        print(encoder_states)\n",
    "        print(h)\n",
    "        combinedState = lambda encoder_state: self.attention(torch.cat((h, encoder_state), 1))\n",
    "        attention_vector = torch.cat(h,encoder_states)\n",
    "        \n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, h = self.cell(output, h)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, h, attention\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, 1, self.h_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
