{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning\n",
    "This file is currently a work in progress. Expect instability, inconsistency, and ugly code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIRECTORY = \"../../data\"\n",
    "\n",
    "# Delete me, eventually\n",
    "data_path = DATA_DIRECTORY + '/dialog-babi-task1/'\n",
    "training_file = data_path + 'dialog-babi-task1-API-calls-trn-workspace.txt'\n",
    "dev_file = data_path + 'dialog-babi-task1-API-calls-dev-workspace.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial object constructors\n",
    "class Workspace():\n",
    "    def __init__(self, tree):\n",
    "        # assert tree is array\n",
    "        self.tree = tree\n",
    "        \n",
    "    # TODO: Fill out body when we need the workspace   \n",
    "    def update(self, action):\n",
    "        if action.what == 'INIT':\n",
    "            return Workspace(['root'])\n",
    "        elif action.what == 'MSG':\n",
    "            return self\n",
    "        elif action.what == 'ADD':\n",
    "            return self\n",
    "    \n",
    "class Action():\n",
    "    def __init__(self, who, what, content=False):\n",
    "        self.who = who\n",
    "        self.what = what\n",
    "        self.content = content\n",
    "        \n",
    "    def to_string(self):\n",
    "        if self.content:\n",
    "            return f'{self.who}: {self.what} {self.content}'\n",
    "        else:\n",
    "            return f'{self.who}: {self.what}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def string_to_action(s):\n",
    "    who = s[0]\n",
    "    i = s.index(\" \")\n",
    "    line = s[i+1:]\n",
    "    j = line.index(\" \")\n",
    "    what = line[0:j]\n",
    "    content = line[j+1:]\n",
    "    return Action(who, what, content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_dialogs(filename):\n",
    "    init_entry = {\n",
    "        \"workspace\": Workspace([]),\n",
    "        \"action\": Action(who='A', what='INIT')\n",
    "    }\n",
    "    \n",
    "    action_list = [init_entry]\n",
    "    current_workspace = init_entry[\"workspace\"]\n",
    "    \n",
    "    for line in open(filename, 'r'):\n",
    "        line = line[:-1]\n",
    "        if not line:\n",
    "            continue\n",
    "        try: \n",
    "            action_list.append({\"workspace\": current_workspace,\n",
    "                                \"action\": string_to_action(line)})\n",
    "        except ValueError:\n",
    "            print(\"This shouldn't happen!\")\n",
    "            return\n",
    "    \n",
    "    return action_list\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo\n",
    "def files_to_words(filenames):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_babi_data():\n",
    "    data_path = DATA_DIRECTORY + '/dialog-babi-task1/'\n",
    "    training_file = data_path + 'dialog-babi-task1-API-calls-trn-workspace.txt'\n",
    "    dev_file = data_path + 'dialog-babi-task1-API-calls-dev-workspace.txt'\n",
    "    types_and_tokens = [\n",
    "            { \"type\": 'cuisine', \"tokens\": ['french', 'italian', 'british', 'spanish', 'indian'] },\n",
    "            { \"type\": 'location', \"tokens\": ['rome', 'london', 'bombay', 'paris', 'madrid'] },\n",
    "            { \"type\": 'price', \"tokens\": ['cheap', 'moderate', 'expensive'] },\n",
    "            { \"type\": 'people', \"tokens\": ['two', 'four', 'six', 'eight'] },\n",
    "        ]\n",
    "    \n",
    "    data = {\n",
    "      \"training\": file_to_dialogs(training_file),\n",
    "      \"dev\": file_to_dialogs(dev_file),\n",
    "      #TODO: words: filesToWords([trainingFile]).concat(specialWords),\n",
    "      #\"typesAndTokens\"\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': <__main__.Action at 0x7f9fac11d208>,\n",
       " 'workspace': <__main__.Workspace at 0x7f9fac11d2b0>}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_babi_data()\n",
    "data['dev'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING!\n",
    "Everything below this cell is currently non-functional. If it compiles, I'll be rather surprised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Adapted from:\n",
    "# http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# TODO: Implement make OneHot\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_size, h_size, n_layers=1):\n",
    "        self.n_layers = n_layers\n",
    "        self.h_size = h_size\n",
    "        self.cell = nn.GRU(x_size, h_size)\n",
    "\n",
    "    # x: input\n",
    "    # h: hidden state\n",
    "    def forward(self, x, h):\n",
    "        for i in range(self.n_layers):\n",
    "            output, h = self.cell(output, h)\n",
    "        return output, h\n",
    "\n",
    "# TODO: Implement initialization\n",
    "\n",
    "\n",
    "# Whether this is a good idea is still in question\n",
    "attentional_net = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(2, 1),\n",
    "                    torch.nn.Tanh())\n",
    "\n",
    "# Whether this is a good idea is still in question\n",
    "decoder_output_net = torch.nn.Sequential(\n",
    "                        torch.nn.Softmax())\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_size, h_size, n_layers=1):\n",
    "        self.n_layers = n_layers\n",
    "        self.h_size = h_size\n",
    "        self.cell = nn.GRU(x_size, h_size)\n",
    "\n",
    "        self.out = nn.Linear(h_size, )\n",
    "        self.attention = torch.nn.Sequential(torch.nn.Linear(2 * x_size, 1),\n",
    "                                             torch.nn.Tanh())\n",
    "\n",
    "    def forward(self, decoder_state, encoder_states):\n",
    "        # Combine State\n",
    "        combinedState = lambda encoder_state, z: self.attention(torch.cat(decoder_state, encoder_state), 0)\n",
    "\n",
    "        # This is terrible and I feel dirty\n",
    "        zero_states = torch.zeros(encoder_states.size()[0])\n",
    "\n",
    "        # Create attention vector\n",
    "        attention_vector = T.softmax(torch.Tensor.map_(encoder_states, zero_states,\n",
    "                                                      combinedState))\n",
    "\n",
    "        final = encoder_states * torch.transpose(attention_vector.unsqueeze(0), 0, 1).expand(encoder_states.size())\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output, h = self.cell(output, h)\n",
    "        return output, h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
